{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ce83171-b4df-40a5-aae3-b0f87cc3c754",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e35f72d0-fcf4-472e-9327-ea7392f8db2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline \n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba6587da-59df-44dc-b364-ab22e639143c",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = pipeline(task=\"translation\",\n",
    "                      model=\"facebook/nllb-200-distilled-600M\",\n",
    "                      torch_dtype=torch.bfloat16)     #bfloat16 to compress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17941bac-982c-4f36-8659-4163c79ba6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\\\n",
    "My puppy is adorable, \\\n",
    "His name is Leo.\n",
    "Leo is friendly.\n",
    "He is also thoughtful. \\\n",
    "We all have nice pets!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be3578b9-0840-4536-a8bb-6641c9298c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_translated = translator(text,\n",
    "                             src_lang=\"eng_Latn\",\n",
    "                             tgt_lang=\"hin_Deva\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46cf2ae8-97ea-4069-966c-6cb003fa48a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': '‡§Æ‡•á‡§∞‡§æ ‡§™‡§ø‡§≤‡•ç‡§≤‡§æ ‡§™‡•ç‡§Ø‡§æ‡§∞‡§æ ‡§π‡•à, ‡§â‡§∏‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§≤‡§ø‡§Ø‡•ã ‡§π‡•à ‡§≤‡§ø‡§Ø‡•ã ‡§¶‡•ã‡§∏‡•ç‡§§‡§æ‡§®‡§æ ‡§π‡•à, ‡§µ‡§π ‡§≠‡•Ä ‡§µ‡§ø‡§ö‡§æ‡§∞‡§∂‡•Ä‡§≤ ‡§π‡•à ‡§π‡§Æ‡§æ‡§∞‡•á ‡§™‡§æ‡§∏ ‡§∏‡§≠‡•Ä ‡§Ö‡§ö‡•ç‡§õ‡•á ‡§™‡§æ‡§≤‡§§‡•Ç ‡§ú‡§æ‡§®‡§µ‡§∞ ‡§π‡•à‡§Ç'}]\n"
     ]
    }
   ],
   "source": [
    "print(text_translated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d94417-eaaf-4411-827e-374da99609e6",
   "metadata": {},
   "source": [
    "<h3> Free up memory by calling garbage collector</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6070a71c-f52a-42bb-a149-2d3f7534a6e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del translator\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ee5fc3-2401-43b0-ad9c-b155cad73b33",
   "metadata": {},
   "source": [
    "<h2>Build the summarization pipeline using ü§ó Transformers Library</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a30df0bc-fcf1-4db3-992b-c856976649bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(task=\"summarization\",\n",
    "                      model=\"facebook/bart-large-cnn\",\n",
    "                      torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b06892d-bc3a-41b3-a0a7-3b8210d0966c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text= \"\"\" Large Language Models (LLMs) have significantly advanced natural language processing, but tokenization-based architectures bring notable limitations. These models depend on fixed-vocabulary tokenizers like Byte Pair Encoding (BPE) to segment text into predefined tokens before training. While functional, tokenization can introduce inefficiencies and biases, particularly when dealing with multilingual data, noisy inputs, or long-tail distributions. Additionally, tokenization enforces uniform compute allocation across tokens, regardless of their complexity, limiting scalability and generalization for diverse data types.Training on byte-level sequences has traditionally been computationally intensive due to the long sequence lengths required. Even with improvements in self-attention mechanisms, tokenization continues to be a bottleneck, reducing robustness and adaptability in high-entropy tasks. These challenges highlight the need for a more flexible and efficient approach.Meta AI‚Äôs Byte Latent Transformer (BLT) seeks to address these issues by eliminating tokenization altogether. \\\n",
    "BLT is a tokenizer-free architecture that processes raw byte sequences and dynamically groups them into patches based on data complexity. \\\n",
    "This approach enables efficient scaling, matching, or exceeding the performance of tokenization-based LLMs while improving robustness and inference efficiency.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "618a60aa-ca1d-4785-8144-0850668bb45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = summarizer(text,\n",
    "                     min_length=100,\n",
    "                     max_length=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c068018b-5794-4ba1-867c-7f84f90fa707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'Large Language Models (LLMs) have significantly advanced natural language processing. These models depend on fixed-vocabulary tokenizers to segment text into predefined tokens. Tokenization can introduce inefficiencies and biases, particularly when dealing with multilingual data, noisy inputs, or long-tail distributions.Meta AI‚Äôs Byte Latent Transformer (BLT) seeks to address these issues by eliminating tokenization altogether. BLT is a tokenizer-free architecture that processes raw byte sequences and dynamically groups them into patches based on data complexity.'}]\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8c58d6-4f4b-4540-9dfe-db7bfc80c4f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
